{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import datetime, json\n",
    "import gc\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import os, multiprocessing\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "pyximport.install(reload_support=True)\n",
    "from mrr import mrr as mrr_cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path, point_allowed_path=False):\n",
    "    split_folder = os.path.split(path)\n",
    "    if not point_allowed_path:\n",
    "        if '.' in split_folder[1]:\n",
    "            # path is a file\n",
    "            path = split_folder[0]\n",
    "    if not os.path.exists(path):\n",
    "        print(f'{path} folder created')\n",
    "        os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "#         print(col)\n",
    "        col_type = df[col].dtype\n",
    "                \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(train_data, val_data, merge_data_path_list, done_files=[], merge_cols='', flag=0):\n",
    "    compulsory_cols = [\"query_id\", \"user_id\", \"session_id\", \"product_id\"]\n",
    "    for path in merge_data_path_list:\n",
    "        print(\"Merging file...\", path.split('/')[-1])\n",
    "        prev_cols_train = set(train_data.columns.tolist())\n",
    "        prev_cols_val = set(val_data.columns.tolist())\n",
    "        d = pd.read_csv(path)\n",
    "#         d = reduce_mem_usage(d)\n",
    "        if 'is_click' in d.columns.tolist():\n",
    "            d = d.drop('is_click', 1)\n",
    "        if flag==0:\n",
    "            merge_cols = [d.columns[0]]\n",
    "            \n",
    "        for col in d.columns.tolist():\n",
    "            if col in train_data.columns.tolist() and col not in compulsory_cols and col not in merge_cols:\n",
    "                d = d.drop(col, 1)\n",
    "            \n",
    "        train_data = train_data.merge(d, on=merge_cols, how='left')\n",
    "        val_data = val_data.merge(d, on=merge_cols, how='left')\n",
    "#         print(\"Train: \", train_data.shape, \"Val: \", val_data.shape)\n",
    "        done_files.append(path)\n",
    "        del d\n",
    "        gc.collect()\n",
    "#     print(\"Train: \", train_data.shape, \"Val: \", val_data.shape)\n",
    "    \n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../data_phase1/train.parquet\")\n",
    "sub = pd.read_parquet(\"../data_phase1/validation.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(value={\"context_type\": \"NA\"})\n",
    "sub = sub.fillna(value={\"context_type\": \"NA\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_df = pd.read_parquet(\"../data_phase1/attributes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(attribute_df, on='product_id', how='left')\n",
    "sub = sub.merge(attribute_df, on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "sub = reduce_mem_usage(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['days_elapsed'] = (train['week'].astype(int)-1)*7 + train['week_day'].astype(int)\n",
    "sub['days_elapsed'] = (sub['week'].astype(int)-1)*7 + sub['week_day'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../preprocessed_data/\"\n",
    "merge_paths = list(set(glob(BASE_PATH+\"*.csv\")) - \\\n",
    "                   set([BASE_PATH+\"session_wise_product_attribute_frequency_agg_features.csv\", \\\n",
    "                        BASE_PATH+\"product_click_features_by_session.csv\", \\\n",
    "                        BASE_PATH+\"product_material_attribute_one-hot.csv\", \\\n",
    "                        BASE_PATH+\"train_product_context_sim.csv\", BASE_PATH+\"val_product_context_sim.csv\",\n",
    "                        BASE_PATH+\"test_product_context_sim.csv\",\n",
    "                        BASE_PATH+\"user_click_percentage_features.csv\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, sub = merge_data(train, sub, merge_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.memory_usage(deep=True).sum()/(1024*1024*1024), sub.memory_usage(deep=True).sum()/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, sub = merge_data(train, sub, [BASE_PATH+\"session_wise_product_attribute_frequency_agg_features.csv\", \\\n",
    "                                     BASE_PATH+\"product_click_features_by_session.csv\"], merge_cols=[\"session_id\", \"product_id\"], flag=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_sim_train = pd.read_csv(BASE_PATH+\"train_product_context_sim.csv\", error_bad_lines=False)\n",
    "pc_sim_sub = pd.read_csv(BASE_PATH+\"val_product_context_sim.csv\")\n",
    "\n",
    "pc_sim_train = pc_sim_train.fillna(0)\n",
    "pc_sim_sub = pc_sim_sub.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_sim_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(pc_sim_train, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n",
    "sub = sub.merge(pc_sim_sub, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"days_elapsed_since_first_user_action\"] = train[\"days_elapsed\"] - train[\"first_user_action_days_elapsed\"]\n",
    "sub[\"days_elapsed_since_first_user_action\"] = sub[\"days_elapsed\"] - sub[\"first_user_action_days_elapsed\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_prod_price_from_user_tier_mean\"] = train[\"product_price\"] - train[\"user_tier_mean_product_price\"]\n",
    "train[\"diff_prod_start_online_date_from_user_tier_mean\"] = train[\"product_price\"] - train[\"user_tier_mean_product_start_online_date\"]\n",
    "\n",
    "sub[\"diff_prod_price_from_user_tier_mean\"] = sub[\"product_price\"] - sub[\"user_tier_mean_product_price\"]\n",
    "sub[\"diff_prod_start_online_date_from_user_tier_mean\"] = sub[\"product_price\"] - sub[\"user_tier_mean_product_start_online_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_start_online_date_from_user_tier_clicked_mean\"] = train[\"start_online_date\"] - train[\"user_tier_mean_click_product_price\"]\n",
    "train[\"diff_start_online_date_from_user_tier_clicked_mean\"] = train[\"start_online_date\"] - train[\"user_tier_mean_click_product_start_online_date\"]\n",
    "\n",
    "sub[\"diff_start_online_date_from_user_tier_clicked_mean\"] = sub[\"start_online_date\"] - sub[\"user_tier_mean_click_product_price\"]\n",
    "sub[\"diff_start_online_date_from_user_tier_clicked_mean\"] = sub[\"start_online_date\"] - sub[\"user_tier_mean_click_product_start_online_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_prod_price_from_session_mean\"] = train[\"product_price\"] - train[\"mean_session_product_price\"]\n",
    "train[\"diff_prod_price_from_query_mean\"] = train[\"product_price\"] - train[\"max_query_price\"]\n",
    "\n",
    "sub[\"diff_prod_price_from_session_mean\"] = sub[\"product_price\"] - sub[\"mean_session_product_price\"]\n",
    "sub[\"diff_prod_price_from_query_mean\"] = sub[\"product_price\"] - sub[\"max_query_price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = json.load(open(\"../preprocessed_data/average_click_values.json\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_prod_price_from_click_mean\"] = train[\"product_price\"] - dic[\"mean_click_price\"]\n",
    "sub[\"diff_prod_price_from_click_mean\"] = sub[\"product_price\"] - dic[\"mean_click_price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_start_online_date_from_session_mean\"] = train[\"start_online_date\"] - train[\"session_start_online_date_mean\"]\n",
    "train[\"diff_start_online_date_from_query_mean\"] = train[\"start_online_date\"] - train[\"mean_query_start_online_date\"]\n",
    "\n",
    "sub[\"diff_start_online_date_from_session_mean\"] = sub[\"start_online_date\"] - sub[\"session_start_online_date_mean\"]\n",
    "sub[\"diff_start_online_date_from_query_mean\"] = sub[\"start_online_date\"] - sub[\"mean_query_start_online_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"diff_start_online_date_from_click_mean\"] = train[\"start_online_date\"] - dic[\"mean_click_start_online_date\"]\n",
    "sub[\"diff_start_online_date_from_click_mean\"] = sub[\"start_online_date\"] - dic[\"mean_click_start_online_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del attribute_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank_features = pd.read_csv(\"../preprocessed_data/rank_features/train_rank_features_and_query_prod_list.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_rank_features = pd.read_csv(\"../preprocessed_data/rank_features/val_rank_features_and_query_prod_list.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank_features = train_rank_features[['query_id', 'user_id', 'session_id', 'product_id', 'price_rank', \\\n",
    "                                           'start_online_date_rank', 'user_step']]\n",
    "sub_rank_features = sub_rank_features[['query_id', 'user_id', 'session_id', 'product_id', 'price_rank', \\\n",
    "                                           'start_online_date_rank', 'user_step']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_rank_features, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n",
    "sub = sub.merge(sub_rank_features, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_features = pd.read_csv(\"../preprocessed_data/user_click_percentage_features.csv\")\n",
    "sub_user_features = pd.read_csv(\"../preprocessed_data/user_click_percentage_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_user_features, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n",
    "sub = sub.merge(sub_user_features, on=[\"query_id\", \"user_id\", \"session_id\", \"product_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../preprocessed_data/clickout_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_paths = glob(BASE_PATH+\"*.csv\")\n",
    "done_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, sub = merge_data(train, sub, merge_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_click = train.is_click.values\n",
    "train = train.drop([\"is_click\"], 1)\n",
    "train[\"is_click\"] = is_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.memory_usage(deep=True).sum()//(1024*1024*1024), sub.memory_usage(deep=True).sum()//(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values([\"query_id\"])\n",
    "sub = sub.sort_values([\"query_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = train.groupby(\"query_id\").count().reset_index()[\"user_id\"].values\n",
    "sub_groups = sub.groupby(\"query_id\").count().reset_index()[\"user_id\"].values\n",
    "sub_groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_groups = train_sample.groupby(\"query_id\").count().reset_index()[\"user_id\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back category to object\n",
    "for col in sub.columns.tolist():\n",
    "    if X.dtypes[col]==\"O\" or \"int\" in str(train.dtypes[col]) or \"float\" in str(train.dtypes[col]):\n",
    "        continue\n",
    "    if sub.dtypes[col]==\"O\" or \"int\" in str(train.dtypes[col]) or \"float\" in str(train.dtypes[col]):\n",
    "        continue\n",
    "#     if X_val.dtypes[col]==\"O\" or \"int\" in str(train.dtypes[col]) or \"float\" in str(train.dtypes[col]):\n",
    "#         continue\n",
    "    if X.dtypes[col]==\"category\":\n",
    "        X[col] = X[col].astype(str)\n",
    "    if sub.dtypes[col]==\"category\":\n",
    "        sub[col] = sub[col].astype(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([X, sub], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = pd.DataFrame(full_data.isnull().sum()).reset_index()\n",
    "nan_df = nan_df.rename(columns={\"index\": \"feature\"})\n",
    "nan_df[\"nan_perc\"] = (nan_df[0]/len(full_data))*100.0\n",
    "remove_cols = nan_df[nan_df[\"nan_perc\"]>=30][\"feature\"].values.tolist()\n",
    "nan_df[nan_df[\"nan_perc\"]>=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = remove_cols + [\"week\", \"week_day\", \"material_values\", \"attribute_values\"]\n",
    "remove_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.memory_usage(deep=True).sum()/(1024*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformations and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols = train.select_dtypes('object').columns.tolist()[4:]\n",
    "\n",
    "cols = X.columns.tolist()\n",
    "cat_cols = cols[3:12] + cols[16:25]\n",
    "cat_cols = cat_cols+['gender']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = MultiColumnLabelEncoder(columns = cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cat_encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = cat_encoder.transform(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder1 = MultiColumnLabelEncoder(columns = ['gender'])\n",
    "X = cat_encoder1.fit_transform(X)\n",
    "X_sub = cat_encoder1.transform(X_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = remove_cols + [\"query_id\", \"session_id\", \"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=train.copy()\n",
    "y = X['is_click'].values\n",
    "X = X.drop(remove_cols+['is_click'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df_filter = pd.DataFrame(X.isnull().sum()).reset_index()\n",
    "nan_df_filter = nan_df_filter.rename(columns={\"index\": \"feature\"})\n",
    "nan_df_filter[\"nan_perc\"] = (nan_df_filter[0]/len(X))*100.0\n",
    "nan_df_filter = nan_df_filter[nan_df_filter[\"nan_perc\"]>0]\n",
    "filter_nan_cols = nan_df_filter[\"feature\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "for col in filter_nan_cols:\n",
    "    if col=='product_context_similarity':\n",
    "        X[col] = X[col].astype(float)\n",
    "        X[col] = X[col].fillna(0)    \n",
    "    else:\n",
    "        X[col] = X[col].astype(float)\n",
    "        X[col] = X[col].fillna(X[col].mean())\n",
    "        dic[col] = X[col].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(dic, open(\"../train_meta/column_mean_imputation.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=reduce_mem_usage(X)\n",
    "X_sub=reduce_mem_usage(X_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, sum(train_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_reciprocal_rank(rs):\n",
    "    '''\n",
    "    rs: 2d array\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "    '''\n",
    "\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "        silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.001, 0.4, 'log-uniform'),\n",
    "        'min_child_weight': (0, 15),\n",
    "        'max_depth': (10, 200),\n",
    "        'max_leaves': (10, 200),\n",
    "        'subsample': (0.6, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.3, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.3, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': (1, 35)\n",
    "    },    \n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bayes_cv_tuner.fit(X, y, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
